{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "using StatsBase\n",
    "using Combinatorics\n",
    "using AbstractTrees\n",
    "using GraphRecipes\n",
    "using Plots\n",
    "\n",
    "default(size=(1000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Node\n",
    "    itemset # ex: ['a', 'b']\n",
    "    children::Vector{Any}\n",
    "    sup \n",
    "    parent \n",
    "\n",
    "    # constructor\n",
    "    Node(its, chi = [], su = 0, par = nothing) = new(its, chi, su, par)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HÀM BỔ TRỢ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "large_itemsets_to_file (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AbstractTrees.children(node::Node) = node.children\n",
    "\n",
    "function AbstractTrees.printnode(io::IO, node::Node)\n",
    "    str = node.itemset\n",
    "    print(io, str)\n",
    "end\n",
    "\n",
    "function tree_to_file(tree, file_path)\n",
    "    file_ins = open(file_path, \"w\") # file instance\n",
    "    maxdepth = 6000 # display maximum number of branches\n",
    "    print_tree(file_ins, tree, maxdepth) # export tree to file\n",
    "    close(file_ins)\n",
    "end \n",
    "\n",
    "function read_database(file_path)\n",
    "    database = []\n",
    "\n",
    "    open(file_path) do f\n",
    "        for (i, line) in enumerate(eachline(f))\n",
    "            row = [parse(Int64, x) for x in split(line)]\n",
    "            push!(database, row)\n",
    "        end\n",
    "    end\n",
    "    return database\n",
    "end\n",
    "\n",
    "function print_candidate_nodes(candidate_nodes)\n",
    "    for node in candidate_nodes\n",
    "        println(\"  $(node.itemset) -> $(node.children)\")\n",
    "    end\n",
    "end\n",
    "\n",
    "function large_itemsets_to_file(all_large_itemset, file_path)\n",
    "    open(file_path, \"w\") do file_ins\n",
    "        for itemset in all_large_itemset\n",
    "            for set in itemset\n",
    "                println(file_ins, set)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREE PROJECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "candidate_nodes_gen (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function candidate_nodes_gen(large_itemset_nodes)\n",
    "    large_itemset = [node.itemset for node in large_itemset_nodes]\n",
    "    candidate_nodes = []\n",
    "    k = length(large_itemset[1]) + 1 # number of items in each candidate\n",
    "\n",
    "    # for every different pair \n",
    "    for i=1:length(large_itemset)\n",
    "        for j=i+1:length(large_itemset)\n",
    "            # if the pair doesn't have the same first k-2 items -> cannot merge to create new candidate \n",
    "            if large_itemset[i][1:k-2] != large_itemset[j][1:k-2]\n",
    "                continue\n",
    "            end\n",
    "\n",
    "            # else, merge the 2 nodes into u \n",
    "            u = union(Set(large_itemset[i]), Set(large_itemset[j]))\n",
    "            # then convert u (Set) to u (Array), sort u to guarantee a lexico order\n",
    "            u_converted = sort!((x->(x)).(u)) \n",
    "\n",
    "            # pruning: \n",
    "            flag = 0 \n",
    "                \n",
    "            for subset in collect(combinations(u_converted,k-1))\n",
    "                # subset generated by combinations() might not be lexicographically ordered so re-sort the subset\n",
    "                sort!(subset) \n",
    "                \n",
    "                # if ∃ subset, subset is not a large itemset \n",
    "                if subset ∉ large_itemset \n",
    "                    flag = 1\n",
    "                    break\n",
    "                end\n",
    "            end\n",
    "            # otherwise, if ∀ subset, subset is a large itemset\n",
    "            if flag == 0\n",
    "                node = Node(u_converted)\n",
    "                node.parent = large_itemset_nodes[i] # form child-parent relationship\n",
    "                push!(large_itemset_nodes[i].children, node) # form parent-child relationship\n",
    "                push!(candidate_nodes, node) # add node to return list \n",
    "            end\n",
    "        end  \n",
    "    end\n",
    "    return candidate_nodes\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apriori_tree (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function apriori_tree(database, min_sup)\n",
    "    min_sup = min_sup * length(database)\n",
    "    one_itemset = sort(collect(countmap(collect(Iterators.flatten(database)))), by=x->x[1]) # lexical sort \n",
    "    one_itemset = [Node([item.first], [], item.second) for item in one_itemset if item.second >= min_sup]\n",
    "    tree = Node([nothing], one_itemset) # init: tree has only 1 root node and some 1-itemset nodes\n",
    "    itr = tree.children # root_node iterator\n",
    "\n",
    "    while !isempty(itr)\n",
    "        println(\"generating candidates...\")\n",
    "        candidate_nodes = candidate_nodes_gen(itr) \n",
    "        \n",
    "        # print message\n",
    "        try\n",
    "            println(length(candidate_nodes[1].itemset))\n",
    "        catch\n",
    "            # pass\n",
    "        end\n",
    "        \n",
    "        # update support of all candidates\n",
    "        for transaction in database\n",
    "            for candidate in candidate_nodes \n",
    "                if candidate.itemset ⊆ transaction # if transaction contains candidate \n",
    "                    candidate.sup += 1\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # filter large itemset nodes, \n",
    "        large_itemset = []\n",
    "        for node in candidate_nodes\n",
    "            if node.sup >= min_sup\n",
    "                push!(large_itemset, node)\n",
    "            else # if there exists an unlarge itemset in tree, delete it from tree \n",
    "                par = node.parent \n",
    "                deleteat!(par.children, findall(x->x==node, par.children))\n",
    "                node.parent = []\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        itr = large_itemset # update itr pointer\n",
    "    end\n",
    "\n",
    "    println(\"Done\")\n",
    "    return tree \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APRIORI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "candidate_gen (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function candidate_gen(large_itemset)\n",
    "    candidate_list = []  # return list\n",
    "    k = length(large_itemset[1]) + 1 # number of items in each candidate\n",
    "\n",
    "    # for every pair in large_itemset\n",
    "    for i=1:length(large_itemset)\n",
    "        for j=i+1:length(large_itemset)\n",
    "            # if the pair doesn't have the same first k-2 items -> cannot merge to create new candidate \n",
    "            if large_itemset[i][1:k-2] != large_itemset[j][1:k-2]\n",
    "                continue\n",
    "            end\n",
    "            \n",
    "            # merge the pair into u \n",
    "            u = union(Set(large_itemset[i]), Set(large_itemset[j]))\n",
    "            # convert u (Set) to u (Array), then sort it to guarantee a lexico order to prevent duplicate \n",
    "            u_converted = sort!((x->(x)).(u)) \n",
    "            \n",
    "            # pruning\n",
    "            flag = 0 \n",
    "            \n",
    "            for subset in collect(combinations(u_converted,k-1))\n",
    "                # subset generated by combinations() might not be lexicographically ordered so re-sort the subset\n",
    "                # ex: ['C','T'] ∈ [['A','B'], ['C', 'T']] while  ['T','C'] ∉ [['A','B'], ['C', 'T']]\n",
    "                sort!(subset) \n",
    "                \n",
    "                # if ∃ subset, subset is not a large itemset \n",
    "                if subset ∉ large_itemset \n",
    "                    flag = 1\n",
    "                    break\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            # otherwise, if all subsets are all large itemsets\n",
    "            if flag == 0\n",
    "                push!(candidate_list, u_converted) # append u_converted to return list \n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return candidate_list\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apriori (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function apriori(database, min_sup)\n",
    "    min_sup = min_sup * length(database)\n",
    "    # init: take all the 1-itemsets that satisfies min_sup\n",
    "    one_itemset = countmap(collect(Iterators.flatten(database)))\n",
    "    one_itemset = collect(keys(filter(kv -> kv.second >= min_sup, one_itemset)))  \n",
    "    one_itemset = sort([[item] for item in one_itemset])\n",
    "    large_itemset = one_itemset \n",
    "    # result list \n",
    "    all_large_itemset = [] \n",
    "    \n",
    "    while(!isempty(large_itemset))\n",
    "        push!(all_large_itemset, large_itemset)\n",
    "        println(\"Generating canididates...\")\n",
    "        candidate_list = candidate_gen(large_itemset) # generate all candidates \n",
    "        freq = Dict(cand=>0 for cand in candidate_list) # init freq to count sup \n",
    "\n",
    "        # print message\n",
    "        try \n",
    "            println(length(collect(candidate_list)[1]))\n",
    "        catch\n",
    "            # pass\n",
    "        end\n",
    "        \n",
    "        # update support of all candidates\n",
    "        for transaction in database\n",
    "            for candidate in candidate_list\n",
    "                if candidate ⊆ transaction\n",
    "                    freq[candidate] += 1\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # filter large itemsets from candidates\n",
    "        large_itemset = collect(keys(filter(kv -> kv.second >= min_sup, freq)))\n",
    "    end \n",
    "\n",
    "    println(\"Done\")\n",
    "    return all_large_itemset\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHẠY THUẬT TOÁN VỚI CÁC DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating canididates..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating canididates...\n",
      "3\n",
      "Generating canididates...\n",
      "4\n",
      "Generating canididates...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating candidates...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating candidates...\n",
      "3\n",
      "generating candidates...\n",
      "4\n",
      "generating candidates...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# sample testcase \n",
    "sample_database = [[\"A\" \"C\" \"T\" \"W\"], \n",
    "                    [\"C\" \"D\" \"W\"],\n",
    "                    [\"A\" \"C\" \"T\" \"W\"],\n",
    "                    [\"A\" \"C\" \"D\" \"W\"],\n",
    "                    [\"A\" \"C\" \"D\" \"T\" \"W\"],\n",
    "                    [\"C\" \"D\" \"T\"]]\n",
    "min_sup = 0.5 \n",
    "res = apriori(sample_database, min_sup)\n",
    "large_itemsets_to_file(res, \"sample_apriori.txt\") \n",
    "# # ------------------------------------------------\n",
    "res = apriori_tree(sample_database, min_sup) \n",
    "tree_to_file(res, \"sample_tree.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = read_database(\"foodmart.txt\")\n",
    "min_sup = 0.000965 # 10 (~15m) -> 0.00241; 4 (~21m) -> 0.000965\n",
    "\n",
    "# output: 1165 large itemsets \n",
    "res = apriori(database, min_sup) \n",
    "large_itemsets_to_file(res, \"foodmart_apriori.txt\") \n",
    "# ------------------------------------------------\n",
    "res = apriori_tree(database, min_sup) \n",
    "tree_to_file(res, \"foodmart_tree.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating canididates..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating canididates...\n",
      "3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating canididates...\n",
      "4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating canididates...\n",
      "5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating canididates...\n",
      "6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating canididates...\n",
      "7\n",
      "Generating canididates...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "database = read_database(\"chess.txt\")\n",
    "min_sup = 0.9\n",
    "\n",
    "# output: 622 large itemsets \n",
    "res = apriori(database, min_sup)\n",
    "large_itemsets_to_file(res, \"chess_apriori.txt\") # export tree to foodmart_op.txt\n",
    "# ------------\n",
    "res = apriori_tree(database, min_sup) \n",
    "tree_to_file(res, \"chess_tree.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating canididates...\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating canididates...\n",
      "3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating canididates...\n",
      "4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating canididates...\n",
      "5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating canididates...\n",
      "6\n",
      "Generating canididates...\n",
      "Done\n",
      "generating candidates...\n",
      "2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generating candidates...\n",
      "3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generating candidates...\n",
      "4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating candidates...\n",
      "5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generating candidates...\n",
      "6\n",
      "generating candidates...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "database = read_database(\"mushrooms.txt\")\n",
    "min_sup = 0.4\n",
    "\n",
    "# output: 505 large itemsets\n",
    "res = apriori(database, min_sup) \n",
    "large_itemsets_to_file(res, \"mushrooms_apriori.txt\") # export tree to foodmart_op.txt\n",
    "# ------------\n",
    "res = apriori_tree(database, min_sup) \n",
    "tree_to_file(res, \"mushrooms_tree.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating canididates...\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating canididates...\n",
      "3\n",
      "Generating canididates...\n",
      "Done"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generating candidates...\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating candidates...\n",
      "3\n",
      "generating candidates...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "database = read_database(\"retail.txt\")\n",
    "min_sup = 0.05\n",
    "\n",
    "# output: ?\n",
    "res = apriori(database, min_sup) \n",
    "large_itemsets_to_file(res, \"retail_apriori.txt\") # export tree to foodmart_op.txt\n",
    "# ------------\n",
    "res = apriori_tree(database, min_sup) \n",
    "tree_to_file(res, \"retail_tree.txt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÁP DỤNG THỰC TẾ\n",
    "- Source: https://www.kaggle.com/irfanasrullah/groceries/version/2\n",
    "- Mô tả file `groceries.csv`: chứa 9835 dòng/transaction, mỗi dòng tương ứng với giỏ hàng của 1 user mua hàng tại cửa hàng\n",
    "- Bài toán: liệt kê các tập phổ biến. Rút luật?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = []\n",
    "\n",
    "open(\"groceries.csv\") do f\n",
    "    for (i, line) in enumerate(eachline(f))\n",
    "        row = [string(x) for x in split(line, \",\")]\n",
    "        push!(database, row)\n",
    "    end\n",
    "end\n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating candidates...\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating candidates...\n",
      "3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generating candidates...\n",
      "4\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "res = apriori_tree(database, 0.01)\n",
    "tree_to_file(res, \"groceries_tree.txt\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.4",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
